[{"body":"Task","id":"src/task.html#task"},{"body":"See  ImagePreprocessing .","id":"src/transforms/imagepreprocessing.html"},{"body":"public   ImagePreprocessing   —   struct Converts an image to a color, then to a 3D - tensor and finally normalizes the values using  means  and  stds . If no  means  or  stds  are given, use ImageNet statistics .","id":"docstrings/DLPipelines.ImagePreprocessing.html"},{"body":"private   shouldbatch   —   function Whether models for  method  take in batches of inputs .  Default is  true .","id":"docstrings/DLPipelines.shouldbatch.html"},{"body":"private   makeitem   —   function Tries to assign a  DataAugmentation.Item  from  data  based on its type . args  are passed to the chosen   Item  constructor . AbstractMatrix{<:Colorant}   - >  Image Vector{<:Union{Nothing, SVector}}   - >  Keypoints","id":"docstrings/DLPipelines.makeitem.html"},{"body":"public   Task   —   type Represents a mapping from high - level types I  to  T . See also  Method .","id":"docstrings/DLPipelines.Task.html"},{"body":"public   ImageClassification   —   struct A  Method  for multi - class image classification using softmax probabilities . categories  is a vector of the category labels .  Alternatively, you can pass an integer . Images are resized to  sz . During training, a random crop is used and  augmentations , a  DataAugmentation.Transform are applied .","id":"docstrings/DLPipelines.ImageClassification.html"},{"body":"Methods DLPipelines . jl  comes with some  Method s already implemented . ImageClassification","id":"docs/methods/methods.html#methods"},{"body":"Transforms Transforms are building blocks that can help you implementing the interfaces for custom  Method s .  Using them is completely optional . SpatialTransforms ImagePreprocessing","id":"docs/transforms.html#transforms"},{"body":"Overview Recall the 4 things needed to start training with  FluxTraining . jl : a model to train, a loss function, an optimizer; and training and validation data iterators For the first three we use: The last part is where  DLPipelines . jl  comes in . As in the  ImageClassification  overview , we first need to load an image classification dataset: Now we construct an  ImageClassification  object with the proper configuration: After splitting the dataset into training and validation split, we can use MethodDataset  to get data containers that will encode the observations . By passing either  Training  or  Validation  as  Context to the datasets, the correct transformations will be automatically applied . For example, a random crop will be used for the  traindataset , while images in  validdataset  will always be cropped from the center . Loading an observation from either will now correctly return a normalized 3D - array with the image data and a one - hot encoded category: To finally get an iterator over batches, we can use  DataLoader  from * DataLoaders . jl . It will also make sure to prefetch the data on background threads so that the training loop isn ’ t slowed down waiting for the next batch of data . Now we can create a  Learner : You can pass any number of callbacks to  Learner , for example  ToGPU()  to utilize your GPU when training . And that ’ s it, you can start training with a call to  fit! : To find out more about what you can do with  FluxTraining . jl , see its documentation .","id":"docs/tutorials/fluxtraining.html#overview"},{"body":"Types input::AbstractMatrix{2, <:Colorant} : an image target::Int  the category that the image belongs to x::AbstractArray{Float32, 3} : a normalized 3D - array with dimensions  height, width, channels y::AbstractVector{Float32} : one - hot encoding of category","id":"docstrings/DLPipelines.ImageClassification.html#types"},{"body":"Types We start off by defining the abstract types that will be used for dispatch .","id":"src/task.html#types"},{"body":"In code The most important type is  Method  which represents a method for a  Task .  All interface functions will dispatch on  Method .  It should be a concrete  struct  containing necessary configuration . Let ’ s give the other concepts variable names and generic types so we can refer to them more easily: task::Task : a task mapping  I  to  T method::Method{Task} : a method implementing a task using encodings  X  and  Y input::I : an input target::T : a target x::X : an encoded input y::Y : an encoded target ŷ::Ŷ : a model output inputs ,  targets ,  xs ,  ys : batches of the respective data model : a function  (X) -> Ŷ lossfn : a function  (Ŷ, Y) -> Number So a  Task  is an abstract type representing a mapping from some input type  I  to target type  T .  A  Method{T}  implements the task  T  by using encoded representations  X  and  Y . As a concrete example, consider image classification: task::ImageClassificationTask : subtype of  Task method::ImageClassification : subtype of  Method{ImageClassificationTask} input::AbstractMatrix{2, <:Colorant} : an image target::Int  the category that the image belongs to x::AbstractArray{Float32, 3} : a normalized 3D - array with dimensions  height, width, channels y::AbstractVector{Float32} : one - hot encoding of category ŷ::AbstractVector{Float32̂} : softmax probabilities","id":"docs/introduction.html#in-code"},{"body":"core interface implementation","id":"src/methods/imageclassification.html"},{"body":"public   encodetarget   —   function Encode  target  into a representation that a model for  task  outputs .","id":"docstrings/DLPipelines.encodetarget.html"},{"body":"Image classification See  ImageClassification ’ s for complete documentation of its arguments Let ’ s explore what you can do with the  Method  interface implemented .  We ’ re using DLDatasets . jl  to access  ImageNette , a small image classification dataset .  Install the package using ]add https://github.com/lorenzoh/DLDatasets.jl With the packages imported, loading the dataset is straightforward: As you can see, every observation consists of an image and a category label: We can also retrieve the 10 different categories from the dataset ’ s metadata:","id":"docs/methods/imageclassification.html#image-classification"},{"body":"Remarks When should I implement  encode  vs .   encodeinput  and  encodetarget ? In simple cases like image classification we can encode the inputs and targets separately and you should prefer  encodeinput  and  encodetarget . The default implementation for  encode  when given an  (input, target) - tuple is to delegate to  encodeinput  and  encodetarget . In other cases like semantic segmentation, however, we want to apply stochastic augmentations to both image and segmentation mask .  In that case you need to encode both at the same time using  encode . Another situation where  encode  is needed is when  sample  is not a tuple of  (input, target) , for example a  Dict  that includes additional information .   encode  still needs to return an  (x, y) - tuple, though .","id":"docstrings/DLPipelines.encode.html#remarks"},{"body":"private   decodeŷ   —   function Decodes a model output into a target .","id":"docstrings/DLPipelines.decodeŷ.html"},{"body":"Image classification  Method With the dataset ready, we can create an instance of  ImageClassification . Note that you could use any image classification dataset, as long as  getobs(ds, idx) returns an image and a category . We can now use this  method  with a  Context  to transform the data . The image is encoded as a normalized 3D - array: And the category is one - hot encoded: You can also use  MethodDataset  to create a wrapper around your existing dataset that directly returns encoded observations:","id":"docs/methods/imageclassification.html#image-classification-method"},{"body":"Core pipelines To give a motivation for the interface, consider the two most important pipelines in a deep learning application: training and inference . Note: I ’ m purposely neglecting batching here (for now), as it doesn ’ t change the semantics of the data pipeline, just the practical implementation . During  inference , we have an input and obtain a target prediction .  Writing this with types gives us: When  training , we first encode both input and target, including any augmentation .  We then feed the encoded input to the model and compare its output with the true encoded target . From those two pipelines, we can extract the following transformations: I -> X  encoding input Ŷ -> T  decoding model output (I, T) -> (X, Y)  encoding input and target These make up the  core interface .","id":"docs/introduction.html#core-pipelines"},{"body":"Setup If you want to follow along, you ’ ll have to install some other libraries; and import the following:","id":"docs/tutorials/fluxtraining.html#setup"},{"body":"Required functions encodeinput (method, context, input) -> x  encodes an input so that it can be fed to a model decodeŷ (method, context, ŷ) -> target  decodes the output of a model into a target either encode (method, context, sample) -> (x, y)  encodes a sample containing input and target so they can be used for a training step encodetarget (method, context, target) -> y encodes a target so that it can be compared to a model output using a loss function","id":"docs/interfaces/core.html#required-functions"},{"body":"Interfaces Find out how to implement custom  Method s . Core interface , for inference and training pipelines . Interpretation interface .","id":"docs/interfaces/overview.html#interfaces"},{"body":"DLPipelines . jl DLPipelines . jl  is an interface for defining deep learning data pipelines .  This includes every data transformation beside the model itself: preprocessing and augmenting the data before feeding it into the model, and decoding the model ’ s outputs to targets . task.jl  defines the  task and method interface . The transforms are defined under  transforms/ : transforms/spatial.jl transforms/imagepreprocessing.jl dataset.jl","id":"src/DLPipelines.html#dlpipelinesjl"},{"body":"DLPipelines DLPipelines . jl  is an interface for defining deep learning data pipelines .  This includes every data transformation beside the model itself: preprocessing and augmenting the data before feeding it into the model, and decoding the model ’ s outputs to targets . With the interface defined, it ’ s dead simple to create data iterators for training and inference pipelines .  See  image classification  as a motivating example . The package was born from the realization that the data pipeline plays a large role in many deep learning projects .  It abstracts the pipeline into steps that lend themselves to building training, inference and other pipelines . Read on here  to find out more about the interface . DLPipelines . jl  is currently in a pre - implementation phase where the focus is on exploring suitable interfaces .","id":"README.html#dlpipelines"},{"body":"Optional functions shouldbatch (method)  defines whether the model for a method should take batches of encoded inputs .  The default is  true .","id":"docs/interfaces/core.html#optional-functions"},{"body":"Introduction In supervised deep learning, we ’ re usually trying to solve a problem by finding a mapping from some inputs to some targets .  Let ’ s call this a  task .  Consider for example this table: Task Input Target Image classification Image Category Semantic segmentation Image Category per pixel Object detection Image Bounding boxes Text completion Text Next character There are usually multiple ways to go about solving a task .  We call a  method  a concrete approach to a task that has a learnable part (the model) but also defines how inputs and targets are  encoded  into a form that can be fed to and output by the model model outputs are  decoded  to obtain target predictions; and os an example method, consider the commmon way of approaching the task of image classification: images are encoded into normalized 3D - Arrays of  Float32 s and categories into one - hot vectors the predicted probability distributions can be decoded into a category by finding the index of the highest score; and the model takes in batches of encoded inputs and output batches of encoded targets An additional complication comes from the fact that the encoding and decoding step may differ based on situation .  For example, during training we often want to apply some augmentation when encoding that would be detrimental to performance during inference .","id":"docs/introduction.html#introduction"},{"body":"Method implementations methods/imageclassification.jl","id":"src/DLPipelines.html#method-implementations"},{"body":"Using with FluxTraining . jl DLPipelines . jl  works great together with FluxTraining . jl . We ’ ll train an image classifier to show how .","id":"docs/tutorials/fluxtraining.html#using-with-fluxtrainingjl"},{"body":"Core interface Next the core interface is defined: encode encodeinput encodetarget decodeŷ decodey shouldbatch","id":"src/task.html#core-interface"},{"body":"public   Context   —   type Represents a context in which a data transformation is made .  This allows using dispatching for varying behavior, for example, to apply augmentations only during training or use non - destructive cropping during inference . Available contexts are  Training ,  Validation  and Inference .","id":"docstrings/DLPipelines.Context.html"},{"body":"public   encodeinput   —   function Encode  input  into a representation that a model for  method takes as input . See also  Method ,  encode , and  encodetarget .","id":"docstrings/DLPipelines.encodeinput.html"},{"body":"See  introduction  and  core interface .","id":"src/task.html"},{"body":"Context  and concrete types","id":"src/task.html#context-and-concrete-types"},{"body":"Core interface First you need to  define a custom  Method  (and possibly a  Task ) . Then implement the following functions by dispatching on the method .","id":"docs/interfaces/core.html#core-interface"},{"body":"Model input size:  (sz..., ch, batch)  where  ch  depends on color type  C . output size:  (nclasses, batch)","id":"docstrings/DLPipelines.ImageClassification.html#model"},{"body":"public   encode   —   function Encode a  sample  containing both input and target  . If  sample  is a  Tuple  of (input, target), the default behavior is to pass them to  encodeinput  and  encodetarget","id":"docstrings/DLPipelines.encode.html"},{"body":"Method","id":"src/task.html#method"},{"body":"Optional interfaces interpretation.jl training.jl","id":"src/DLPipelines.html#optional-interfaces"},{"body":"Remarks What is the  context  argument? We often want to apply data augmentation during training but not during validation or inference .  You can dispatch on  context  to define different behavior for the different situations .  See  Context  to see the available options . When should I implement  encode  vs .   encodetarget ? See  encode .","id":"docs/interfaces/core.html#remarks"},{"body":"public   Method   —   parametric type Represents a concrete approach for solving a Task . See  core interface  for more on how to implement custom  Method s","id":"docstrings/DLPipelines.Method.html"},{"body":"private   decodey   —   function Decodes an encoded target back into a target . Defaults to using  decodeŷ","id":"docstrings/DLPipelines.decodey.html"},{"body":"See  SpatialTransforms .","id":"src/transforms/spatial.html"},{"body":"public   SpatialTransforms   —   struct Transformation that resizes images and keypoints to  size . In context  Training , applies  augmentations .","id":"docstrings/DLPipelines.SpatialTransforms.html"}]